import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, learning_curve, validation_curve
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc
from sklearn.preprocessing import LabelEncoder

# Load the dataset
CPS = pd.read_csv("CPS.csv")

# Filter data: Remove individuals aged 17 or below, irrelevant codes in 'UHRSWORKLY', and non-relevant DIFFANY codes
CPS = CPS[(CPS['AGE'] > 17) & (~CPS['UHRSWORKLY'].isin([999, 997])) & (~CPS['DIFFANY'].isin([0]))]

# Create a binary variable for Child Tax Credit receipt (CTCCRD_dummy)
CPS['CTCCRD_dummy'] = CPS['ACTCCRD'].apply(lambda x: 1 if x > 0 else 0)

# Check the first few rows of the data to ensure cleaning steps were successful
CPS.head()

# Summary statistics of key variables
print(CPS[['UHRSWORKLY', 'ACTCCRD', 'AGE', 'EITCRED', 'HEALTH']].describe())

# Function to calculate and print skewness and kurtosis for key variables
def calc_skew_kurt(data, var):
    skew = data[var].skew()
    kurtosis = data[var].kurt()
    print(f"Skewness of {var}: {skew}")
    print(f"Kurtosis of {var}: {kurtosis}")

# Calculate skewness and kurtosis for key variables
for var in ['UHRSWORKLY', 'ACTCCRD', 'AGE', 'EITCRED', 'HEALTH']:
    calc_skew_kurt(CPS, var)

# Frequency table of CTCCRD_dummy
ctc_freq_table = CPS['CTCCRD_dummy'].value_counts(normalize=True) * 100
ctc_count_table = CPS['CTCCRD_dummy'].value_counts()
ctc_summary = pd.DataFrame({'Count': ctc_count_table, 'Percentage': ctc_freq_table})
print(ctc_summary)

# Frequency table of DIFFANY
diffany_freq_table = CPS['DIFFANY'].value_counts(normalize=True) * 100
diffany_count_table = CPS['DIFFANY'].value_counts()
diffany_summary = pd.DataFrame({'Count': diffany_count_table, 'Percentage': diffany_freq_table})
print(diffany_summary)

# Plot: Distribution of Usual Hours Worked per Week (excluding outliers > 100)
sns.histplot(CPS['UHRSWORKLY'][CPS['UHRSWORKLY'] <= 100], bins=20)
plt.title("Distribution of Usual Hours Worked Per Week (2021)")
plt.xlabel("Hours Worked Per Week")
plt.ylabel("Frequency")
plt.show()

# Binning 'UHRSWORKLY' variable into categories
bins = [0, 40, 80, 100]
labels = ['0-40', '41-80', '81-100']
CPS['UHRSWORKLY_binned'] = pd.cut(CPS['UHRSWORKLY'], bins=bins, labels=labels, right=False)

# Grouped data: Distribution of Hours Worked by Child Tax Credit Receipt
grouped_data = CPS.groupby(['CTCCRD_dummy', 'UHRSWORKLY_binned'], observed=False).size().unstack(fill_value=0)
ax = grouped_data.plot(kind='bar', stacked=True, figsize=(10, 6), cmap='coolwarm')
plt.title("Distribution of Hours Worked by Additional Child Tax Credit Receipt (2021)")
plt.xlabel("Additional CTC Receipt (1 = Received, 0 = Did Not Receive)")
plt.ylabel("Count of Individuals")
plt.legend(title="Hours Worked", bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

# Plot: Distribution of Additional Child Tax Credit Amount
sns.histplot(CPS['ACTCCRD'])
plt.title("Distribution of Additional Child Tax Credit Amount (2021)")
plt.xlabel("Additional Child Tax Credit Amount")
plt.ylabel("Frequency")
plt.show()

# Prepare feature matrix (X) and target variable (y)
X = CPS[['ACTCCRD', 'CTCCRD_dummy', 'EITCRED', 'AGE', 'HEALTH', 'DIFFANY']] 
y = CPS['UHRSWORKLY']

# Split the data into training and testing sets
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, random_state=1)
print(f'X train shape: {Xtrain.shape}, y train shape: {ytrain.shape}')

# Initialize and fit the Linear Regression model
model = LinearRegression(fit_intercept=True)
model.fit(Xtrain, ytrain)

# Coefficients for each feature
coefficients = pd.DataFrame(model.coef_, X.columns, columns=['Coefficient'])
print(coefficients)

# Evaluate model performance on training and test data (R² score)
train_score = model.score(Xtrain, ytrain)
test_score = model.score(Xtest, ytest)
print(f'Training data R² score: {train_score:.4f}')
print(f'Test data R² score: {test_score:.4f}')

# Visualize Linear Regression model: Plot 'ACTCCRD' vs. 'UHRSWORKLY'
model.fit(Xtrain[['ACTCCRD']], ytrain)
plt.scatter(Xtrain['ACTCCRD'], ytrain, color='blue', label='Data')
plt.plot(Xtrain['ACTCCRD'], model.predict(Xtrain[['ACTCCRD']]), color='red', label='Regression Line')
plt.xlabel("Additional Child Tax Credit Amount (ACTCCRD)")
plt.ylabel("Hours Worked Per Week (UHRSWORKLY)")
plt.title("Regression of Hours Worked (Labor Supply) on Additional Child Tax Credit in 2021", pad=20)
plt.legend()
plt.figtext(0.5, -0.1, "Note: Data sourced from the CPS (Current Population Survey) ASEC (Annual Social and Economic Supplement) 2022.", 
            ha="center", fontsize=7)
plt.show()

# Binning 'UHRSWORKLY' variable into categories
bins = [0, 40, 80, 100]  
labels = ['0-40', '41-80', '81-100']

# Create a new column for binned hours worked
CPS['UHRSWORKLY_binned'] = pd.cut(CPS['UHRSWORKLY'], bins=bins, labels=labels, right=False)

# Check the distribution of binned hours worked

# Prepare feature matrix (X) and target variable (y)
X = CPS[['ACTCCRD', 'CTCCRD_dummy', 'EITCRED', 'AGE', 'HEALTH', 'DIFFANY']]  
y = CPS['UHRSWORKLY_binned']  # Use the binned version of UHRSWORKLY

# Split the data into training and testing sets
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=42)

# Decision Tree with a maximum depth of 5
tree = DecisionTreeClassifier(max_depth=5)
tree.fit(Xtrain, ytrain)

# Evaluate model performance on training and test data 
train_score = tree.score(Xtrain, ytrain)
test_score = tree.score(Xtest, ytest)

print(f"Training accuracy: {train_score:.4f}")
print(f"Test accuracy: {test_score:.4f}")

# Plot the decision tree
plt.figure(figsize=(12, 12))
plot_tree(tree, filled=True, feature_names=Xtrain.columns, class_names=labels, rounded=True, fontsize=10)
plt.title("Decision Tree Classifier (Max Depth = 5) for Binned Hours Worked")
plt.show()

# Feature Importances from Decision Tree
plt.bar(Xtrain.columns, tree.feature_importances_)
plt.xlabel('Feature')
plt.ylabel('Importance')
plt.title('Feature Importances')
plt.xticks(rotation=60)
plt.show()

cv_scores = cross_val_score(DecisionTreeClassifier(), X, y, cv=5)
print(f"Cross-validation scores: {cv_scores}")

# Define cross-validation strategy
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Set the range of max_depth values to test
param_range = np.arange(1, 20)

# Generate validation curve data
train_scores, test_scores = validation_curve(
    DecisionTreeClassifier(), X, y,
    param_name="max_depth", param_range=param_range,
    cv=cv, n_jobs=-1
)

# Calculate mean and standard deviation for train and test scores
train_mean = np.mean(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
train_std = np.std(train_scores, axis=1)
test_std = np.std(test_scores, axis=1)

# Calculate mean and standard deviation for train and test scores
train_mean = np.mean(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
train_std = np.std(train_scores, axis=1)
test_std = np.std(test_scores, axis=1)

# Generate learning curve data
train_sizes, train_scores, test_scores = learning_curve(
    DecisionTreeClassifier(), X, y, cv=5, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10)
)

# Calculate mean and standard deviation for train and test scores
train_mean = np.mean(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
train_std = np.std(train_scores, axis=1)
test_std = np.std(test_scores, axis=1)

# Plot learning curve
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, train_mean, label='Training score', color='blue', linestyle='-', marker='o')
plt.plot(train_sizes, test_mean, label='Cross-validation score', color='red', linestyle='-', marker='x')
plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color='blue', alpha=0.2)
plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color='red', alpha=0.2)
plt.title('Learning Curve for Decision Tree Classifier')
plt.xlabel('Training Size')
plt.ylabel('Accuracy Score')
plt.legend()
plt.show()
